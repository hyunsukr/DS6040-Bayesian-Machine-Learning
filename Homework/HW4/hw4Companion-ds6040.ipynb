{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb8acbb-f6c5-48be-a228-5b59702880dd",
   "metadata": {},
   "source": [
    "__Homework 4 Companion Notebook__\n",
    "\n",
    "Here you will find the various functions and skeleton code for use in completing HW4. Take care to carefully read and understand the provided code before use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6c2b3-ff78-4e11-9266-e90c2d837f94",
   "metadata": {},
   "source": [
    "__Hierarchical Regression__\n",
    "\n",
    "This is an example that shows how to build a hierarchical model. These examples are adapted from https://docs.pymc.io/en/v3/pymc-examples/examples/generalized_linear_models/GLM-hierarchical.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce1474-b240-4173-8ba2-abedf92a213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "from pymc3 import get_data\n",
    "\n",
    "# Import radon data (Preprocessing specific to this dataset.)\n",
    "srrs2 = pd.read_csv(get_data('srrs2.dat'))\n",
    "srrs2.columns = srrs2.columns.map(str.strip)\n",
    "srrs_mn = srrs2[srrs2.state=='MN'].copy()\n",
    "srrs_mn['fips'] = srrs_mn.stfips*1000 + srrs_mn.cntyfips\n",
    "cty = pd.read_csv(get_data('cty.dat'))\n",
    "cty_mn = cty[cty.st=='MN'].copy()\n",
    "cty_mn[ 'fips'] = 1000*cty_mn.stfips + cty_mn.ctfips\n",
    "srrs_mn = srrs_mn.merge(cty_mn[['fips', 'Uppm']], on='fips')\n",
    "srrs_mn = srrs_mn.drop_duplicates(subset='idnum')\n",
    "u = np.log(srrs_mn.Uppm)\n",
    "n = len(srrs_mn)\n",
    "srrs_mn.county = srrs_mn.county.map(str.strip)\n",
    "mn_counties = srrs_mn.county.unique()\n",
    "counties = len(mn_counties)\n",
    "county_lookup = dict(zip(mn_counties, range(len(mn_counties))))\n",
    "county = srrs_mn['county_code'] = srrs_mn.county.replace(county_lookup).values\n",
    "radon = srrs_mn.activity\n",
    "srrs_mn['log_radon'] = log_radon = np.log(radon + 0.1).values\n",
    "floor_measure = srrs_mn.floor.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db46c25-a482-4436-b187-16e356eb9a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81abc02-4e9e-41e5-9d8e-f915b7fb0024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34bd68a-017a-4050-8147-a249f8b1f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Priors for the fixed effects\n",
    "    # a - overall intercept, level of radon at mean levels of uranium and no basement\n",
    "    mu_a = pm.Normal('mu_a', mu=0., sd=1e5)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', 5)\n",
    "    \n",
    "    #b - overall effect of having a basement\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sd=1e5)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "    \n",
    "    #c - overall effect of uranium\n",
    "    mu_c = pm.Normal('mu_c', mu=0., sd=1e5)\n",
    "    sigma_c = pm.HalfCauchy('sigma_c', 5)\n",
    "    \n",
    "    # Random intercepts as offsets\n",
    "    a_offset = pm.Normal('a_offset', mu=0, sd=1, shape=counties)\n",
    "    a = pm.Deterministic(\"a\", mu_a + a_offset * sigma_a)\n",
    "    \n",
    "    # County level effect of basement as offset\n",
    "    b_offset = pm.Normal('b_offset', mu=0, sd=1, shape=counties)\n",
    "    b = pm.Deterministic(\"b\", mu_b + b_offset * sigma_b)\n",
    "    \n",
    "    # County level effect of uranium as offset\n",
    "    c_offset = pm.Normal('c_offset', mu=0, sd=1, shape=counties)\n",
    "    c = pm.Deterministic(\"c\", mu_c + c_offset * sigma_c)\n",
    "    \n",
    "    # Residual Error\n",
    "    sigma_y = pm.HalfCauchy('sigma_y', 5)\n",
    "\n",
    "    # This is the specification for the regression equation itself.\n",
    "    y_hat = a[county] + b[county]*floor_measure + c[county]*u\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sd=sigma_y, observed=log_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5a8da-bc7f-4a3d-8e38-e763e302c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(1000, n_init=50000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c834ae-898d-4461-b63b-30552bcd42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(hierarchical_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccfb59-39e7-4104-8e9a-b9601e7e7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forest Plots can be quite useful in visualizing the posterior distributions across the clustering units.\n",
    "from pymc3 import forestplot, traceplot, plot_posterior\n",
    "plt.figure(figsize=(6,14))\n",
    "forestplot(hierarchical_trace, var_names=['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fca87d-1dc9-438f-ab30-c9f0e9c5f0d1",
   "metadata": {},
   "source": [
    "__Part 2: Bayesian Model Averaging__\n",
    "\n",
    "Below you will find the BMA_Wine class. There are a couple of differences between it and the LDA/QDA class. First, you have to fit the model after you construct it (i.e. BMA = BMA_Wine(outcomevariable, datawithoutoutcomevariable); BMA_fit = BMA.fit()). Second, all output functions require handing it the true class values as well as the data you want to predict from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbdffe-6ce2-4b8d-a30e-22531961fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from itertools import combinations\n",
    "mp.dps = 50\n",
    "\n",
    "#This class is based on the BMA class provided by Bill Basener in: https://www.kaggle.com/billbasener/bayesian-model-averaging-logistic-regression\n",
    "#It has been modified to allow for multinomial regression (logistic regression for more than 2 categories)\n",
    "#Specifically, I've hardcoded the model as a 3 category multinomial regression, so this code doesn't generalize to any other model\n",
    "class BMA_Wine:\n",
    "    \n",
    "    def __init__(self, y, X, **kwargs):\n",
    "        # Setup the basic variables.\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.names = list(X.columns)\n",
    "        self.nRows, self.nCols = np.shape(X)\n",
    "        self.likelihoods = mp.zeros(self.nCols,1)\n",
    "        self.likelihoods_all = {}\n",
    "        self.coefficients_mp = mp.zeros(self.nCols,2)\n",
    "        self.coefficients = np.zeros((self.nCols, 2))\n",
    "        self.probabilities = np.zeros(self.nCols)\n",
    "        # Check the max model size. (Max number of predictor variables to use in a model.)\n",
    "        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n",
    "        if 'MaxVars' in kwargs.keys():\n",
    "            self.MaxVars = kwargs['MaxVars']\n",
    "        else:\n",
    "            self.MaxVars = self.nCols  \n",
    "        # Prepare the priors if they are provided.\n",
    "        # The priors are provided for the individual regressor variables.\n",
    "        # The prior for a model is the product of the priors on the variables in the model.\n",
    "        if 'Priors' in kwargs.keys():\n",
    "            if np.size(kwargs['Priors']) == self.nCols:\n",
    "                self.Priors = kwargs['Priors']\n",
    "            else:\n",
    "                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n",
    "                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n",
    "                self.Priors = np.ones(self.nCols)  \n",
    "        else:\n",
    "            self.Priors = np.ones(self.nCols)  \n",
    "        if 'Verbose' in kwargs.keys():\n",
    "            self.Verbose = kwargs['Verbose'] \n",
    "        else:\n",
    "            self.Verbose = False \n",
    "        if 'RegType' in kwargs.keys():\n",
    "            self.RegType = kwargs['RegType'] \n",
    "        else:\n",
    "            self.RegType = 'LS' \n",
    "        \n",
    "    def fit(self):\n",
    "        # Perform the Bayesian Model Averaging\n",
    "        \n",
    "        # Initialize the sum of the likelihoods for all the models to zero.  \n",
    "        # This will be the 'normalization' denominator in Bayes Theorem.\n",
    "        likelighood_sum = 0\n",
    "        \n",
    "        # To facilitate iterating through all possible models, we start by iterating thorugh\n",
    "        # the number of elements in the model.  \n",
    "        max_likelihood = 0\n",
    "        for num_elements in range(1,self.MaxVars+1): \n",
    "            \n",
    "            if self.Verbose == True:\n",
    "                print(\"Computing BMA for models of size: \", num_elements)\n",
    "            \n",
    "            # Make a list of all index sets of models of this size.\n",
    "            Models_current = list(combinations(list(range(self.nCols)), num_elements)) \n",
    "             \n",
    "            # Occam's window - compute the candidate models to use for the next iteration\n",
    "            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod/20)\n",
    "            # Models_next:     the set of candidate models for the next iteration\n",
    "            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n",
    "            #                    to a model from Models_previous\n",
    "                                   \n",
    "            \n",
    "            # Iterate through all possible models of the given size.\n",
    "            for model_index_set in Models_current:\n",
    "                \n",
    "                # Compute the linear regression for this given model. \n",
    "                model_X = self.X.iloc[:,list(model_index_set)]\n",
    "\n",
    "                model_regr = sm.MNLogit(self.y, model_X).fit(disp=0)\n",
    "                \n",
    "                # Compute the likelihood (times the prior) for the model. \n",
    "                model_likelihood = mp.exp(-model_regr.bic/2)*np.prod(self.Priors[list(model_index_set)])\n",
    "                \n",
    "                if self.Verbose == True:\n",
    "                    pass\n",
    "                    #print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n",
    "                self.likelihoods_all[str(model_index_set)] = model_likelihood\n",
    "                \n",
    "                # Add this likelihood to the running tally of likelihoods.\n",
    "                likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n",
    "                # Add this likelihood (times the priors) to the running tally\n",
    "                # of likelihoods for each variable in the model.\n",
    "                for idx, i in zip(model_index_set, range(num_elements)):\n",
    "                    self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=2000)\n",
    "                    for j in np.arange(model_regr.params.shape[1]):\n",
    "\n",
    "                        self.coefficients_mp[idx,j] = mp.fadd(self.coefficients_mp[idx,j], model_regr.params[j][i]*model_likelihood, prec=2000)\n",
    "                max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n",
    "                \n",
    "                    \n",
    "\n",
    "        # Divide by the denominator in Bayes theorem to normalize the probabilities \n",
    "        # sum to one.\n",
    "        self.likelighood_sum = likelighood_sum\n",
    "        for idx in range(self.nCols):\n",
    "            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n",
    "            for j in range(2):\n",
    "                self.coefficients[idx,j] = mp.fdiv(self.coefficients_mp[idx,j],likelighood_sum, prec=1000)\n",
    "        \n",
    "        # Return the new BMA object as an output.\n",
    "        return self\n",
    "    \n",
    " \n",
    "    def predict_MAP(self,true_class, data):\n",
    "        data = np.asarray(data)\n",
    "        result = np.zeros((data.shape[0],3))\n",
    "        temp = sm.MNLogit(true_class, exog=np.asarray(data))\n",
    "        result = temp.predict(params = self.coefficients, exog = np.asarray(data))\n",
    "        result = pd.DataFrame(result, columns= [\"A\", \"C\", \"F\"])\n",
    "        res_MAP = result.idxmax(axis=1)\n",
    "        to_return = pd.DataFrame({'TrueClass':true_class, 'MAP':res_MAP})\n",
    "        return to_return\n",
    "    \n",
    "\n",
    "    def misclass_rate(self, true_class, data):\n",
    "        maps = self.predict_MAP(true_class, data)\n",
    "        \n",
    "        \n",
    "        maps['Mis_class'] = maps['MAP']  == maps['TrueClass']\n",
    "        \n",
    "        mis_class =  1 - maps['Mis_class'].mean()\n",
    "        \n",
    "        return mis_class\n",
    "    def misclass_xtabs(self, true_class, data):\n",
    "        maps = self.predict_MAP(true_class, data)\n",
    "        \n",
    "        xtabs = pd.crosstab(maps['MAP'], maps['TrueClass'])        \n",
    "        return xtabs\n",
    "    \n",
    "    def summary(self):\n",
    "        # Return the BMA results as a data frame for easy viewing.\n",
    "        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n",
    "             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5279d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
